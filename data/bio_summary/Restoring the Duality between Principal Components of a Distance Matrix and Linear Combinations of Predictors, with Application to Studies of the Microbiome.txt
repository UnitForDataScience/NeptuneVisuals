whatever scaling and centering is applied the data matrix x can always be written using the singular value decompo- sition svd asx 14 lsrtd1thwhere l is a n x q matrix with orthonormal columns s is a q x q diagonal matrix having posi- tive entries and r is a p x q matrix with orthonormal columns where q is rank of x. if we are willing to measure similarity between observations using d  xxt then the columns of l com- prise the coordinates of the observations in a principal components analysis or a principal coordinates analysis pcoa if count data in x have been scaled and centered as described above since the columns of l are also the principal components of xxt.

because d is real and symmetric we can always write d  bebt where b is orthogonal and e is diagonal however the elements of e may not all be positive unless d is euclidean.we can attempt to restore the relationship between the eigenvectors of d and linear com- binations of the rows of x in two ways either using the singular value decomposition of x as our guide or using prediction of the left singular vectors of x that are used for ordination as our guide.

if x is centered then the total sum of squares is proportional to thevariance of the xijs.lemma 2. let b be a n x d-dimensional matrix with orthonormal columns and let d be ad x d-dimensional diagonal matrix chosen to minimize jjx - bdvtjj2 .

thus if x contains the untransformed counts or even if the data matrix is scaled by the library size for each observation in the limit this corresponds to usingxij 14 pijm or xij 14 pij if we scale the rows of x by the library sizes where pij is the frequency of the jth otu in the ith sample and m is the number of reads selected in each rarefaction.since centering for pcoa is also linear in the elements of x this argument suggests that using the empirical frequencies without rarefaction at least for the decomposition approaches is warranted.turning now to the relationship between the orthogonal regression and decomposition approaches the objective function for the orthogonal regression approach given in assigns equal importance weight to the prediction of each column of b. if we choose to weight the prediction of the jth column of b by d2 a measure of the importance of thejth column then it is easy to show that minimizing the resulting objective functionjjdxvd-1 - bthdjj2 14 jjxv - bdjj2 yields the same values of v and d as minimizing theobjective function for orthogonal decomposition.

however once a distance is calculated it is difficult to know which species or otus contribute to the observed distances or to place future observations in an ordination plot to see if they cluster with the 'correct' group.in high-dimensional data important linear combinations of features are frequently obtained by calculating the pcs of xt x the correlation or covariance matrix of the data depending on how x is scaled.

finally we conclude with a brief discussion.duality between a distance matrix and linear combinations of otusduality and the singular value decompositiondata from a 16s rrna microbiome experiment can be summarized in a n x p-dimensional data matrix x where n is the number of observations and p is the number of species or otus.

the elements of x count the number of reads in observation i that fall into otu j. the row sums referred to as the library size are thought to be largely ancillary thus the count data in x is often converted to otu frequencies by dividing the counts in each row by the corre- sponding library size to put each row on the same scale and then data for each otu is cen- tered by subtracting the mean otu frequency.

we first note a lemma governing minimizers of lemma 1. let w minimize f dwth 14 jjx - bwtjj2 where x has rank q b has dimension n xd and x  lsrt is the singular value decomposition given in .

tobacco type in the tobacco data considered here.appendix proofs of the lemmasproof of lemma 1. because q is the rank of x p 2 q. if p  q the result is trivial since the col- umns of r span rp and so the columns of any p x d-dimensional matrix w can be expressed as a linear combination of columns of r which establishes the result.

these pipelines produce otu counts abundances that can be summarized in a data matrix x here we take the rows to correspond to observations and the columns to species or otus.

these pcs can also be obtained from a singular value decom- position svd of the data matrix x which yields a set of singular vectors for observations and a set of singular vectors for features here otus or species.

equivalently we can first calculate the pcs of xt x to obtain r the pcs of the covariance or correlation depending on scaling matrix of otus.

thus xvdud-1 may give poor prediction of b in the sense that is large while bdruvt may be a poor approximation to x in the sense that is large.because vru 614 vdu otus selected as important for regression may not correspond to important variables for decomposition or vice versa.

in order to ensure that the otus selected are important for both regression and decomposition we next consider minimizing eqs or subject to the constraintvtv 14 i d9thunless all the singular values of x are equal i.e.

while this separation is useful in showing that otus vary systematically across groups investigators often wish to know which otus contribute most to this separation.

