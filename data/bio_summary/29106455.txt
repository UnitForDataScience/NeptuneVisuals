The ViraPipe relies on data parallel computation where the NGS reads are parti- tioned into blocks and processed in parallel.

The whole process includes parallel decompression read interleaving BWA-MEM read alignment filtering and normalizing of non-human reads De novo contigs assembling and searching of sequences with BLAST and HMMER3 tools.Contact Availability and implementation 1 IntroductionThe metagenomics  in the era of Next Generation Sequencing NGS enables the complete sequencing of all microbiological sequences that may be present in a sample.

The index database for the local BLAST alignment can be distributed and queried in parallel if the input data is relatively small and the input query is replicated.

The pipeline itself is based on identification of protein coding sequences with hidden mar- kov models and searching of matches from multiple databases.ViraPipe is specifically tuned for mining virus related genomes as well as detection of novel potentially virus-related sequences in metagenomic datasets generated by NGS technologies.ImplementationViraPipe utilizes data parallel computation strategy as genomic data can be processed in partitions at many levels divided by chromo- somes chromosomal regions and short read partitions and this approach enables us to use existing bioinformatics tools and algo- rithms in the pipeline.

The well-known BLAST search is executed in parallel per contig partitions to query the assembled contigs from the BLAST human genomic and nt databases.

The normalization is executed on Spark over parallel read data partitions.

The input contig data is read from the HDFS in FASTA format produced by MegaHit assembler and the contigs are repartitioned with Spark RDD for parallel BLAST search.

The HDFS partition size is constant 128 MB and the memory allocation for BWA-MEM alignment task was 30 GB at maximum thus the parallel BWA execution is limited to 170 tasks each task was run on single core with 5.12 TB of clus- ter RAM.

The run- time depends on the dataset size of a sample as assembly is per- formed in parallel per sample.

The data was partitioned into 100 partitions and each partition was queried in parallel executors with hmmsearch algorithm using 10 threads.Finally we run an experiment by increasing the amount of Spark executors with 13 samples dataset 105.5 GB.

