for deseqdeseq2 poor performance may be due to the model's assumption that differentially abundant otus are not a large portion of the population  or the model's overdispersion estimates  because most researchers want to infer ecosystem taxon relative abundances from sampling this indicates a large previously unsolved problem in differential abundance testing we also investigated the performance of the techniques on real null data in which there should be no true posi- tives since samples from the same biological group were randomly divided into two groups  section.dirichlet- multinomial40 samples group3000 reads sampledirichlet- multinomial40 samples group50000 reads sampledirichlet- multinomial5 samples group3000 reads sampledirichlet- multinomial5 samples group50000 reads samplegamma- poisson40 samples group3000 reads samplegamma- poisson40 samples group50000 reads samplegamma- poisson5 samples group3000 reads samplegamma- poisson5 samples group50000 reads samplemultinomial40 samples group3000 reads samplemultinomial40 samples group50000 reads samplemultinomial5 samples group3000 reads samplemultinomial5 samples group50000 reads sample0.750.500.250.000.750.500.250.000.750.500.250.000.750.500.250.00normalization methodmodelnone proportion rarefied0.750.500.250.000.750.500.250.000.750.500.250.001.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20 1.25 5 20effect sizefig.

this problem is exacerbated by the observation that the full range of species is rarely saturated so that more bacterial species are observed with more sequencing similar trends by sequencing depth hold for discovery of genes in shotgun metagenomic samples  thus samples with relatively few sequences may have inflated beta b or between sample diversity since authen- tically shared otus are erroneously scored as unique to samples with more sequences second most otu tables are sparse meaning that they contain a high propor- tion of zero counts 90  this sparsity implies that the counts of rare otus are uncertain since they are at the limit of sequencing detection ability when there are many sequences per sample i.e.

the highly sequenced samples appear more similar to each other than the shallowly sequenced samples because the highly sequenced samples are scored as sharing the same rare taxa it should be noted that when groupings in a pcoa plot do not appear to reflect artifactual features this does not necessarily indicate that they reflect relevant biology because it is trivial to generate data with no biological signal that nonetheless form or appear to form clusters.to assess the seven proposed normalization methods shown in table we first examined prior simulations .

conversely it has since been confirmed that low-depth samples contain a higher proportion of contaminants rrna not from the intended sample because the higher depth samples that rarefying keeps may be higher quality and therefore give rarefying an unfair advantage additional file  figure s1 compares clustering accuracy for all the techniques based on the same set of samples remaining in the rarefied dataset.on the real datasets nonparametric multivariateanova permanova was calculated by fitting a type i sequential sums of squares in the linear model ylibrarysize  biologicaleffect.

deseq2 without addition of a constant increased sensitivity on smaller datasets 20 samples per group but tends towards a higher false discovery rate with more samples very uneven 10x library sizes andor compositional effects.

samples with total counts below the defined threshold are excluded sometimes leading researchers to face difficult trade-offs between sampling depth and the number of samples evaluated.

often a particular quantile of the data is used for normalization but choosing the most effective quantile is difficult - furthermore while microbiome data are frequently sparse as discussed above scaling can overestimate or underestimate the prevalence of zero fractions depending on whether zeros are left in or thrown out of the scaling  this is because putting all samples of varying sampling depth on the same scale ignores the differences in sequencing depth and therefore resolution of species caused by differing library sizes between the samples.

if all tech- niques are run on the same samples as those used when rarefying the rarefying technique clusters as many samples into their biological groupings as the alternatives fig.

this practice of removing low-depth samples from the analysisis supported by the recent discovery that small biomass samples are of poorer quality and contain a higher propor- tion of contaminating sequences  .

these results demonstrate that previous microbiome ordinations using rarefying as a normalization method likely clustered similarly compared to newer techniques especially if some low-depth samples were removed.for unweighted metrics that are based on species presence and absence like binary jaccard and unweighted unifrac the variance-stabilizing transformation performed by deseq clusters many fewer samples according to biological origin than other techniques.

this adjusts the matrix counts using a log-like transformation in the nb generalized linear model glm such that the variance in an otu's counts across samples is approximately independent of its meanedger-tmmtrimmed mean by m-values tmm--the tmm scaling factor is calculated as the weighted mean of log-ratios between each pair of samples after excluding the highest count otus and otus with the largest log-fold change.

if the samples from the healthy patients have a 10x larger library size otus of all mean abundance levels will be found to be differentially abundant simply because they may have 10x the number of counts in the healthy patient samples such systematic bias can happen if for example healthy vs. diseased patients are sequenced on separate sequencing runs or are being compared in a meta-analysis.

these simu- lated samples done in triplicate for each combination of parameters were then used to assess normalizationmethods by the proportion of samples correctly classi- fied into the two clusters by the partitioning around medioids pam algorithm mcmurdie and holmes evaluated clustering accuracy with five normalization methods none propor- tion rarefying with replacement as in the multinomial model deseqvs  and uq-logfc in the edger package  and six beta-diversity metrics euclidean bray-curtis poissondist  top-msd  un- weighed unifrac and weighted unifrac  we modified the normalization methods to those in table none proportion rarefying without replacement as in the hypergeometric model css loguq deseqvs and edger-tmm and the beta diversity metrics to those in fig.

here we evaluate how these challenges impact the performance of existing normalization methods and differential abundance analyses.results effects on normalization most normalization methods enable successful clustering of samples according to biological origin when the groups differ substantially in their overall microbial composition.

rarefying more clearly clusters samples according to biological origin than other normalization techniques do for ordinationmetrics based on presence or absence.

