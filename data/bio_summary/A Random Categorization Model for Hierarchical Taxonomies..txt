For a model that always divides into a fixed number j of subcategories producing a j-nary tree rather than a binary tree the mean and variance of b the number of branchings separating a random leaf from the root for a tree with q leaves are j q 1 ujq H - H where H is a harmonic number andj - 1 j - 1 j - 172 j 0 q 0 1 sj q j - 1 ps 1  - ps2 j - 12 j - 1j - 1j ps1  q - psj - 11  1 8where psmz is a polygamma function of order m. If the probabilities are equally divided among the child cate- gories so that p  j-b the mean and variance of the log probabilities are related to the above results simply by factors of ln j and ln j2 respectively see Eq.

From this and the definition of the Gamma function we derive a recursion relation for the probabilityPb q  1  q - 1 Pb q  2 Pb - 1 qq  1q  14with boundary conditions P0 1  1 P0 q  0 for q  0.

We could also achieve a similar smeared distribution by assigning probabilities to each child category according to some continuous distribution rather than equally or by dividing categories into a randomly varying number of child categories.Of course b is itself a random variable that depends on the particular leaf and the structure of the random tree.Given a random tree with q leaves the probability that a randomly chosen leaf has depth b is given byPb q  2-b Cq - 1 b .Gq  12Here C is an unsigned Stirling number of the first kind which obeys the recursion relationCn  1 k  nCn k  Cn k - 13with boundary conditions Cn 0 0 for n  0 and Cn k 0 for k  0 with Cn n 1 for n  0.

To do so we notice that given a tree with q leaves after multinomial sampling with n items we expect to get the following fraction of empty categoriesqfe q n   Pb q 1 - 2-bn b16with Pb q given by eq.

The success of the random model is quite remarkable given the model's lack of adjustable parameters and suggests that the decisions made in designing hierarchical categoriza- tion systems follow a general pattern that our random categorization model is capturing.As a test we also compare the model to four categorizations that are not hierarchical taxonomies and where we would therefore not expect it to perform well populations of German cities U.S. counties world countries and the number of speakers per language.

This recursion relation allows an efficient numerical calculation of the probabilities for arbitrary b q values.This distribution is approximately normal in b near its maximum with variance s2  q2j - 4j2  -3.4  2 ln q and mean u  q2j -0.85  2 ln qqj2qj2 where the approximations are valid atlarge q.

We repeat this procedure until we reach the desired number of categories q leaves of the tree which may be larger than k see below for details on how q is determined.To each leaf we assign a probability p  2-bxX where b is the depth of the leaf the number of divisions separating that leaf from the root x is a uniform random variable between -1 and 1 and X is a normalization factor that ensures the probabilities sum to one.

If we wish to obtain an expected number k of nonempty categories we should choose q by solving the nonlinear equation k  q1 - feq n. We use the value qk n that solves this equation for the number of categories we generate to model a dataset with given k n.These steps can be thought of as generating a random partition of n into q.

The size of the dot scales with qk a measure of how poorly-sampled the dataset is that is strongly correlated with W Pearson correlation between logqk and logW is -0.9532 with p-value 1.98 x 10-5 Spearman correlation between logqk and logW is -0.5829 with p-value 0.077. c The datasets used in this analysis.

The number of categories in each dataset is k the total number of items is n and the number of categories in the tree used to model the data is q so that qk is our model'sestimate of the ratio of the total number of categories to the number of non-empty categories.

